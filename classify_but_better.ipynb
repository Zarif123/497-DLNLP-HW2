{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ],
      "metadata": {
        "id": "AkEzGPQ4XZoC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "onPXFTz0Xek_",
        "outputId": "4808fb79-60c3-4f2b-ae4f-7cba2abced7c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.26.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, BertModel, GPT2LMHeadModel, GPT2Tokenizer\n",
        "import torch.optim as optim\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import time\n",
        "import sys\n",
        "import json\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "CHkzfMQmXXhG"
      },
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train Loop"
      ],
      "metadata": {
        "id": "FL3jJ6QeI1hZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_loop(model, linear, optimizer, criterion, tokenizer, train, num_choices, epochs):\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"Epoch {epoch + 1} ///////////////////////////////\")\n",
        "\n",
        "        train_len = len(train)\n",
        "        total_loss = 0.0\n",
        "\n",
        "        for train_i in range(train_len):\n",
        "            observation = train[train_i]\n",
        "            contexts = []\n",
        "            labels = []\n",
        "            mask = torch.zeros((4, 2), dtype=float).to(device) ##### if code doesnt work, change to numpy\n",
        "\n",
        "            for choice_i in range(num_choices):\n",
        "                context = observation[choice_i][0]\n",
        "                label = observation[choice_i][1]\n",
        "                contexts.append(context)\n",
        "                labels.append(label)\n",
        "                mask[choice_i][label] = 1\n",
        "            \n",
        "            inputs = tokenizer(contexts, max_length=256, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
        "            # inputs = inputs.to(device)\n",
        "            inputs['input_ids'] = inputs['input_ids'].to(device)\n",
        "            inputs['token_type_ids'] = inputs['token_type_ids'].to(device)\n",
        "            inputs['attention_mask'] = inputs['attention_mask'].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            hidden = model(**inputs)\n",
        "\n",
        "            logits = torch.matmul(hidden.last_hidden_state[:, 0, :], linear)\n",
        "\n",
        "            # labels = torch.tensor(labels).to(device)\n",
        "            # loss = criterion(logits, labels)\n",
        "            # # print(loss.item())\n",
        "\n",
        "            probs = F.softmax(logits, dim=1)\n",
        "            correct_probs = probs * mask\n",
        "            log_probs = torch.log(torch.sum(correct_probs, dim=1))\n",
        "            loss = -torch.sum(log_probs)\n",
        "            # print(loss)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            if train_i % 1000 == 0:\n",
        "                print(train_i, \"/\", train_len)\n",
        "        \n",
        "        average_loss = total_loss / train_len\n",
        "        print(f\"Average Loss: {average_loss}\")\n",
        "\n",
        "        torch.save(model.state_dict(), 'classify.pth')\n",
        "            \n"
      ],
      "metadata": {
        "id": "rptL9oaWJBaV"
      },
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test Loop"
      ],
      "metadata": {
        "id": "DlER2Y9WXoOF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_loop(model, linear, tokenizer, test, num_choices):\n",
        "    test_len = len(test)\n",
        "    running_accuracy = 0\n",
        "\n",
        "    for test_i in range(test_len):\n",
        "        observation = test[test_i]\n",
        "        contexts = []\n",
        "        labels = []\n",
        "        mask = torch.zeros((4, 2), dtype=float).to(device) ##### if code doesnt work, change to numpy\n",
        "\n",
        "        for choice_i in range(num_choices):\n",
        "            context = observation[choice_i][0]\n",
        "            label = observation[choice_i][1]\n",
        "            contexts.append(context)\n",
        "            labels.append(label)\n",
        "            mask[choice_i][label] = 1\n",
        "        \n",
        "        inputs = tokenizer(contexts, max_length=256, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
        "        inputs = inputs.to(device)\n",
        "\n",
        "        hidden = model(**inputs)\n",
        "\n",
        "        logits = torch.matmul(hidden.last_hidden_state[:, 0, :], linear)\n",
        "        probs = F.softmax(logits, dim=1)[:, 1]\n",
        "        labels = torch.tensor(labels).to(device)\n",
        "\n",
        "        # print(probs)\n",
        "        # print(labels)\n",
        "\n",
        "        pred = torch.argmax(probs)\n",
        "        real = torch.argmax(labels)\n",
        "        if pred == real:\n",
        "            running_accuracy += 1\n",
        "        \n",
        "    average_accuracy = running_accuracy / test_len\n",
        "    return average_accuracy\n",
        "    # print(f\"Average Accuracy: {average_accuracy}\")\n"
      ],
      "metadata": {
        "id": "NQoTgTYDXn1c"
      },
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Find Device"
      ],
      "metadata": {
        "id": "5hL7weSZK9Rs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "7GppVyVCK_5_"
      },
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Main"
      ],
      "metadata": {
        "id": "klPY3DvMI6Ff"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {
        "id": "KD28eTADH207"
      },
      "outputs": [],
      "source": [
        "def main():  \n",
        "    torch.manual_seed(0)\n",
        "    answers = ['A','B','C','D']\n",
        "\n",
        "    train = []\n",
        "    test = []\n",
        "    valid = []\n",
        "    \n",
        "    file_name = 'train_complete.jsonl'        \n",
        "    with open(file_name) as json_file:\n",
        "        json_list = list(json_file)\n",
        "    for i in range(len(json_list)):\n",
        "        json_str = json_list[i]\n",
        "        result = json.loads(json_str)\n",
        "        \n",
        "        base = result['fact1'] + ' [SEP] ' + result['question']['stem']\n",
        "        ans = answers.index(result['answerKey'])\n",
        "        \n",
        "        obs = []\n",
        "        for j in range(4):\n",
        "            text = base + result['question']['choices'][j]['text'] + ' [SEP]'\n",
        "            if j == ans:\n",
        "                label = 1\n",
        "            else:\n",
        "                label = 0\n",
        "            obs.append([text,label])\n",
        "        train.append(obs)\n",
        "        \n",
        "        # print(obs)\n",
        "        # print(' ')\n",
        "        \n",
        "        # print(result['question']['stem'])\n",
        "        # print(' ',result['question']['choices'][0]['label'],result['question']['choices'][0]['text'])\n",
        "        # print(' ',result['question']['choices'][1]['label'],result['question']['choices'][1]['text'])\n",
        "        # print(' ',result['question']['choices'][2]['label'],result['question']['choices'][2]['text'])\n",
        "        # print(' ',result['question']['choices'][3]['label'],result['question']['choices'][3]['text'])\n",
        "        # print('  Fact: ',result['fact1'])\n",
        "        # print('  Answer: ',result['answerKey'])\n",
        "        # print('  ')\n",
        "                \n",
        "    file_name = 'dev_complete.jsonl'        \n",
        "    with open(file_name) as json_file:\n",
        "        json_list = list(json_file)\n",
        "    for i in range(len(json_list)):\n",
        "        json_str = json_list[i]\n",
        "        result = json.loads(json_str)\n",
        "        \n",
        "        base = result['fact1'] + ' [SEP] ' + result['question']['stem']\n",
        "        ans = answers.index(result['answerKey'])\n",
        "        \n",
        "        obs = []\n",
        "        for j in range(4):\n",
        "            text = base + result['question']['choices'][j]['text'] + ' [SEP]'\n",
        "            if j == ans:\n",
        "                label = 1\n",
        "            else:\n",
        "                label = 0\n",
        "            obs.append([text,label])\n",
        "        valid.append(obs)\n",
        "        \n",
        "    file_name = 'test_complete.jsonl'        \n",
        "    with open(file_name) as json_file:\n",
        "        json_list = list(json_file)\n",
        "    for i in range(len(json_list)):\n",
        "        json_str = json_list[i]\n",
        "        result = json.loads(json_str)\n",
        "        \n",
        "        base = result['fact1'] + ' [SEP] ' + result['question']['stem']\n",
        "        ans = answers.index(result['answerKey'])\n",
        "        \n",
        "        obs = []\n",
        "        for j in range(4):\n",
        "            text = base + result['question']['choices'][j]['text'] + ' [SEP]'\n",
        "            if j == ans:\n",
        "                label = 1\n",
        "            else:\n",
        "                label = 0\n",
        "            obs.append([text,label])\n",
        "        test.append(obs)\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "    model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "    optimizer = optim.Adam(model.parameters(), lr=3e-5)\n",
        "    criterion = nn.NLLLoss()\n",
        "    linear = torch.rand(768, 2)\n",
        "    \n",
        "    model = model.to(device)\n",
        "    linear = linear.to(device)\n",
        "#    Add code to fine-tune and test your MCQA classifier.\n",
        "           \n",
        "\n",
        "    # Use to toggle between training and testing\n",
        "    is_training = False\n",
        "    is_zero_shot = True\n",
        "\n",
        "    if is_training:\n",
        "        train_loop(model, linear, optimizer, criterion, tokenizer, train, 4, 5)\n",
        "    else:\n",
        "        if not is_zero_shot:\n",
        "            model.load_state_dict(torch.load('classify.pth'))\n",
        "            model.eval()\n",
        "\n",
        "        av_valid_acc = test_loop(model, linear, tokenizer, valid, 4)\n",
        "        print(f\"Valid Average Accuracy: {av_valid_acc}\")\n",
        "\n",
        "        av_test_acc = test_loop(model, linear, tokenizer, test, 4)\n",
        "        print(f\"Test Average Accuracy: {av_test_acc}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PdvhAG-7IWUi",
        "outputId": "e3ec76a2-a328-4a96-d419-c959227ba369"
      },
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Valid Average Accuracy: 0.222\n",
            "Test Average Accuracy: 0.194\n"
          ]
        }
      ]
    }
  ]
}